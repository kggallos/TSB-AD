# -*- coding: utf-8 -*-
# Author: Konstantinos Gallos <kggallos@gmail.com>
# License: Apache-2.0 License
"""
This code is adapted from [pythresh] by [KulikDM]
Original source: [https://github.com/KulikDM/pythresh]
"""

import pandas as pd
import numpy as np
import scipy.stats as stats
import argparse, time

from TSB_AD.evaluation.metrics import get_metrics
from TSB_AD.utils.slidingWindows import find_length_rank
from TSB_AD.models.base import BaseDetector

from .thresholding_utils import check_scores, normalize

class BOOT(BaseDetector):
    r"""BOOT class for Bootstrapping thresholder.

       Use a bootstrapping based method to find a non-parametric means
       to threshold scores generated by the decision_scores where outliers
       are set to any value beyond the mean of the confidence intervals.
       See :cite:`martin2006boot` for details

       Parameters
       ----------
       random_state : int, optional (default=1234)
            Random seed for bootstrapping a confidence interval. Can also be set to None.

       Attributes
       ----------

        threshold_ : float
            The threshold value that separates inliers from outliers.

        decision_scores_: ndarray of shape (n_samples,) #TODO
            Not actually used, present for API consistency by convention.
            It contains 0s and 1s because this is a thresholding method.

       Notes
       -----

       The two sided bias-corrected and accelerated bootstrap confidence interval
       is calculated with a confidence level of 0.95. The statistic calculating
       the confidence interval is the standard deviation of the decision
       scores, with the statistic treating corresponding elements of the
       samples in the decision scores as paired

       The returned upper and lower confidence intervals are used to threshold
       the decision scores. Outliers are set to any value above the mean of the
       upper and lower confidence intervals.

       Examples
       --------
       The effects of randomness can affect the thresholder's output performance
       significantly. Therefore, to alleviate the effects of randomness on the
       thresholder a combined model can be used with different random_state values.


    """

    def __init__(self, random_state=1234, normalize=True):
        super().__init__()
        self.random_state = random_state
        self.normalize = normalize

    def fit(self, X, y=None):
        """Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        pass

    def decision_function(self, X):
        """    
        Not used, present for API consistency by convention.
        """
        pass
        
    def predict(self, X):
        """
        Predict anomalies in a batch of data points.

        Parameters
        ----------
        X : numpy array of shape (n_samples,)
            The input data points.

        Returns
        -------
        preds : numpy array of shape (n_samples,)
            Predictions (1 for anomaly, 0 for normal).
        """
        n_samples, n_features = X.shape

        X = check_scores(X, random_state=self.random_state)

        if self.normalize: X = normalize(X)

        limit1, limit2 = stats.bootstrap(
            X.reshape(1, -1),
            np.std,
            paired=True,
            random_state=self.random_state
        ).confidence_interval

        self.threshold_ = (limit1+limit2)/2

        preds = np.zeros(n_samples, dtype=int)
        preds[X >= self.threshold_] = 1

        return preds


if __name__ == '__main__':

    Start_T = time.time()
    ## ArgumentParser
    parser = argparse.ArgumentParser(description='Running MAD')
    parser.add_argument('--filename', type=str, default='001_NAB_id_1_Facility_tr_1007_1st_2014.csv')
    parser.add_argument('--data_direc', type=str, default='Datasets/TSB-AD-U/')
    parser.add_argument('--AD_Name', type=str, default='MAD')
    args = parser.parse_args()

    # multivariate
    # parser.add_argument('--filename', type=str, default='057_SMD_id_1_Facility_tr_4529_1st_4629.csv')
    # parser.add_argument('--data_direc', type=str, default='Datasets/TSB-AD-M/')

    Custom_AD_HP = {
        'random_state': 1234,   # not related to method itself, but to formatting input
    }

    df = pd.read_csv(args.data_direc + args.filename).dropna()
    data = df.iloc[:, 0:-1].values.astype(float)
    label = df['Label'].astype(int).to_numpy()
    print('data: ', data.shape)
    print('label: ', label.shape)

    slidingWindow = find_length_rank(data, rank=1)
    train_index = args.filename.split('.')[0].split('_')[-3]
    data_train = data[:int(train_index), :]
    data_test = data[int(train_index):, :]
    label_test = label[int(train_index):]

    start_time = time.time()

    print("------- ON TEST DATA -------")
    clf = BOOT(**Custom_AD_HP)
    # clf.fit(data_train)
    output = clf.predict(data_test)
    pred = output   # output has already the predictions

    end_time = time.time()
    run_time = end_time - start_time

    evaluation_result = get_metrics(output, label_test, slidingWindow=slidingWindow, pred=pred)
    print('Evaluation Result: ', evaluation_result)

    ####!
    print("------- ON WHOLE DATA -------")
    clf = BOOT(**Custom_AD_HP)
    # clf.fit(data)
    output = clf.predict(data)
    pred = output
    evaluation_result = get_metrics(output, label, slidingWindow=slidingWindow, pred=pred)
    print('Evaluation Result: ', evaluation_result)
