# -*- coding: utf-8 -*-
# Author: Konstantinos Gallos <kggallos@gmail.com>
# License: Apache-2.0 License
"""
This code is adapted from [pythresh] by [KulikDM]
Original source: [https://github.com/KulikDM/pythresh]
"""

import pandas as pd
import numpy as np
import scipy.stats as stats
import argparse, time

from TSB_AD.evaluation.metrics import get_metrics
from TSB_AD.utils.slidingWindows import find_length_rank

from .thresholding_utils import check_scores, normalize

class MCST():
    r"""MCST class for Monte Carlo Shapiro Tests thresholder.

       Use uniform random sampling and statistical testing to evaluate a
       non-parametric means to threshold scores generated by the decision_scores
       where outliers are set to any value beyond the minimum value left after
       iterative Shapiro-Wilk tests have occurred. Note** accuracy decreases with
       array size. For good results the should be array<1000. However still this
       threshold method may fail at any array size.
       See :cite:`coin2008mcst` for details.

       Parameters
       ----------

       random_state : int, optional (default=1234)
            Random seed for the uniform distribution. Can also be set to None.

       Attributes
       ----------

        threshold_ : float
            The threshold value that separates inliers from outliers.

        decision_scores_: ndarray of shape (n_samples,) #TODO
            Not actually used, present for API consistency by convention.
            It contains 0s and 1s because this is a thresholding method.

       Notes
       -----

       The Shapiro-Wilk test is a frequentist statistical test for normality.
       It is used to test the null-hypothesis that the decision scores came
       from a normal distribution. This test statistic is defined as:

       .. math::

          W = \frac{\left(\sum_{i=1}^n a_i x_{(i)} \right)^2}{\sum_{i=1}^n \left(x_i - \bar{x} \right)^2} \mathrm{,}

       where :math:`\bar{x}` is the mean of the scores and :math:`x_{(i)}`
       is the ith-smallest number in the sample (kth order statistic). The
       coefficients :math:`a_i` is given by:

       .. math::

          (a_1,...,a_n) = \frac{m^{\top}V^{-1}}{\sqrt{m^{\top}V^{-1}V^{-1}m}} \mathrm{,}

       where the vector :math:`m=\lvert(m_1,...,m_n \rvert)^{\top}` and :math:`V`
       is the covariance matrix of the order statistics.

       The threshold is set by first calculating an initial Shapiro-Wilk test
       p-value on the decision scores. Using Monte Carlo simulations, random values
       between 0-1 are inserted into the normalized decision scores and p-values are
       calculated. if the p-value is higher than the initial p-value, the initial p-value
       is set to this value and the random value is stored. The minimum stored random
       value is set as the threshold as it is the minimum found outlier.

       Examples
       --------
       The effects of randomness can affect the thresholder's output performance
       significantly. Therefore, to alleviate the effects of randomness on the
       thresholder a combined model can be used with different random_state values.
       E.g.

    """

    def __init__(self, random_state=1234, normalize=True):
        self.random_state = random_state
        self.normalize = normalize

    def fit(self, X, y=None):
        """Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        pass

    def decision_function(self, X):
        """    
        Not used, present for API consistency by convention.
        """
        pass
        
    def predict(self, X):
        """
        Predict anomalies in a batch of data points.

        Parameters
        ----------
        X : numpy array of shape (n_samples,)
            The input data points.

        Returns
        -------
        preds : numpy array of shape (n_samples,)
            Predictions (1 for anomaly, 0 for normal).
        """
        n_samples, n_features = X.shape

        X = check_scores(X, random_state=self.random_state)

        if self.normalize: X = normalize(X)

        # Get Baseline Shapiro-Wilk test p-value
        p_std = stats.shapiro(X).pvalue

        # Create random dataset to insert and test p-values
        rnd = stats.uniform.rvs(loc=0, scale=1, size=len(
            X), random_state=self.random_state)
        rnd = normalize(rnd)
        povr = []

        # Iterate and add a new random variable
        # Perform a Shapiro-Wilk test and see if the new
        # distribution has a lower or higher p-value
        # If higher record these potential outlier values
        for i in range(len(rnd)):

            arr = np.append(X, rnd[i])
            p_check = stats.shapiro(arr).pvalue

            if p_check > p_std:

                p_std = p_check
                povr.append(rnd[i])

        limit = np.min(povr) if povr else 1.1
        self.threshold_ = limit

        preds = np.zeros(n_samples, dtype=int)
        preds[X >= self.threshold_] = 1

        return preds


if __name__ == '__main__':

    Start_T = time.time()
    ## ArgumentParser
    parser = argparse.ArgumentParser(description='Running MCST')
    parser.add_argument('--filename', type=str, default='001_NAB_id_1_Facility_tr_1007_1st_2014.csv')
    parser.add_argument('--data_direc', type=str, default='Datasets/TSB-AD-U/')
    parser.add_argument('--AD_Name', type=str, default='MCST')
    args = parser.parse_args()

    # multivariate
    # parser.add_argument('--filename', type=str, default='057_SMD_id_1_Facility_tr_4529_1st_4629.csv')
    # parser.add_argument('--data_direc', type=str, default='Datasets/TSB-AD-M/')

    Custom_AD_HP = {
        'random_state': 1234,   # not related to method itself, but to formatting input
    }

    df = pd.read_csv(args.data_direc + args.filename).dropna()
    data = df.iloc[:, 0:-1].values.astype(float)
    label = df['Label'].astype(int).to_numpy()
    print('data: ', data.shape)
    print('label: ', label.shape)

    slidingWindow = find_length_rank(data, rank=1)

    clf = MCST(**Custom_AD_HP)
    output = clf.predict(data)
    pred = output
    evaluation_result = get_metrics(output, label, slidingWindow=slidingWindow, pred=pred)
    print('Evaluation Result: ', evaluation_result)
