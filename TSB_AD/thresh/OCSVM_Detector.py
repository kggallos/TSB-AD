# -*- coding: utf-8 -*-
# Author: Konstantinos Gallos <kggallos@gmail.com>
# License: Apache-2.0 License
"""
This code is adapted from [pythresh] by [KulikDM]
Original source: [https://github.com/KulikDM/pythresh]
"""

import pandas as pd
import numpy as np
import argparse, time
import scipy.stats as stats
from sklearn.kernel_approximation import AdditiveChi2Sampler
from sklearn.linear_model import RidgeCV, SGDOneClassSVM
from sklearn.metrics import mean_squared_error
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.svm import OneClassSVM

from TSB_AD.evaluation.metrics import get_metrics
from TSB_AD.utils.slidingWindows import find_length_rank

from .thresholding_utils import check_scores, normalize, gen_kde

class OCSVM():
    """OCSVM class for One-Class Support Vector Machine thresholder.

       Use a one-class svm to evaluate a non-parametric means
       to threshold scores generated by the decision_scores where outliers
       are determined by the one-class svm using a polynomial kernel
       with the polynomial degree either set or determined by regression
       internally. See :cite:`barbado2022ocsvm` for details.

       Parameters
       ----------

       model : {'poly', 'sgd'}, optional (default='sgd')
           OCSVM model to apply

           - 'poly':  Use a polynomial kernel with a regular OCSVM
           - 'sgd':   Used the Additive Chi2 kernel approximation with a SGDOneClassSVM

       degree : int, optional (default='auto')
           Polynomial degree to use for the one-class svm.
           Default 'auto' finds the optimal degree with linear regression

       gamma : float, optional (default='auto')
           Kernel coefficient for polynomial fit for the one-class svm.
           Default 'auto' uses 1 / n_features

       criterion : {'aic', 'bic'}, optional (default='bic')
           regression performance metric. AIC is the Akaike Information Criterion,
           and BIC is the Bayesian Information Criterion. This only applies
           when degree is set to 'auto'

       nu : float, optional (default='auto')
           An upper bound on the fraction of training errors and a lower bound
           of the fraction of support vectors. Default 'auto' sets nu as the ratio
           between the any point that is less than or equal to the median plus
           the absolute difference between the mean and geometric mean over the
           the number of points in the entire dataset

       tol : float, optional (default=1e-3)
           The stopping criterion for the one-class svm

       random_state : int, optional (default=1234)
            Random seed for the SVM's data sampling. Can also be set to None.

       Attributes
       ----------

        threshold_ : float
            The threshold value that separates inliers from outliers.

        decision_scores_: ndarray of shape (n_samples,) #TODO
            Not actually used, present for API consistency by convention.
            It contains 0s and 1s because this is a thresholding method.

       Examples
       --------
       The effects of randomness can affect the thresholder's output performance
       significantly. Therefore, to alleviate the effects of randomness on the
       thresholder a combined model can be used with different random_state values.
       E.g.

    """

    def __init__(self, model='sgd', degree='auto', gamma='auto',
                 criterion='bic', nu='auto', tol=1e-3, 
                 random_state=1234, normalize=True):

        self.model = model
        self.degree = degree
        self.gamma = gamma
        self.crit = criterion
        self.nu = nu
        self.tol = tol
        self.random_state = random_state
        self.normalize = normalize

    def fit(self, X, y=None):
        """Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        pass

    def decision_function(self, X):
        """    
        Not used, present for API consistency by convention.
        """
        pass
        
    def predict(self, X):
        """
        Predict anomalies in a batch of data points.

        Parameters
        ----------
        X : numpy array of shape (n_samples,)
            The input data points.

        Returns
        -------
        preds : numpy array of shape (n_samples,)
            Predictions (1 for anomaly, 0 for normal).
        """
        n_samples, n_features = X.shape

        X = check_scores(X, random_state=self.random_state)

        if self.normalize: X = normalize(X)

        # Get auto nu calculation
        if self.nu == 'auto':

            np.seterr(divide='ignore')
            gmean = stats.gmean(X)
            mean = np.mean(X)
            med = np.median(X)

            self.nu = len(X[X <= med +
                          abs(mean-gmean)])/len(X)

        self.nu = 0.5 if self.nu == 1.0 else self.nu

        # Get auto degree calculation
        if (self.degree == 'auto') & (self.model == 'poly'):

            self.degree = self._auto_crit(X)

        X = X.reshape(-1, 1)

        # Create a one-class svm
        if self.model == 'poly':
            clf = OneClassSVM(gamma=self.gamma, kernel='poly',
                              degree=self.degree, nu=self.nu,
                              tol=self.tol).fit(X)
        else:
            transform = AdditiveChi2Sampler()
            sgd = SGDOneClassSVM(nu=self.nu,
                                 random_state=self.random_state)
            clf = make_pipeline(transform, sgd)
            clf.fit(X)

        # Predict inliers and outliers
        res = clf.predict(X)

        res[res == -1] = 0

        # Remove outliers from the left tail (precaution step)
        X = np.squeeze(X)
        mask = np.where(X <= np.mean(X))
        res[mask] = 0

        self.threshold_ = None

        return res  # TODO preds == res ?
        preds = np.zeros(n_samples, dtype=int)
        preds[X >= self.threshold_] = 1
        return preds

    def _auto_crit(self, decision):
        """Decide polynomial degree using criterion."""

        # Generate kde
        kde, dat_range = gen_kde(decision, 0, 1, len(decision))

        # Set polynomial degrees to test
        polys = [2, 3, 4, 5, 6, 7, 8, 9, 10]
        n = len(decision)

        decision = decision.reshape(-1, 1)
        kde = kde.reshape(-1, 1)

        scores = []

        for poly in polys:

            # Calculate the polynomial features for the kde
            poly_features = PolynomialFeatures(degree=poly, include_bias=True)
            poly_fit = poly_features.fit_transform(kde)

            # Use regression to fit the polynomial
            poly_reg = RidgeCV(alphas=np.logspace(-1, 2, 100))
            poly_reg.fit(poly_fit, dat_range)
            poly_pred = poly_reg.predict(poly_fit)

            # Get the mse and apply the regression performance metric
            mse = mean_squared_error(dat_range, poly_pred)

            if self.crit == 'aic':
                scores.append(n*np.log(mse) + 2*(poly+1))
            else:
                scores.append(n*np.log(mse) + (poly+1)*np.log(n))

        # Set degree from smallest metric score

        return polys[np.argmin(scores)]


if __name__ == '__main__':

    Start_T = time.time()
    ## ArgumentParser
    parser = argparse.ArgumentParser(description='Running OCSVM')
    parser.add_argument('--filename', type=str, default='001_NAB_id_1_Facility_tr_1007_1st_2014.csv')
    parser.add_argument('--data_direc', type=str, default='Datasets/TSB-AD-U/')
    parser.add_argument('--AD_Name', type=str, default='OCSVM')
    args = parser.parse_args()

    Custom_AD_HP = {
        'random_state': 1234,   # not related to method itself, but to formatting input
        'model': 'sgd',
        'degree': 'auto',
        'gamma': 'auto',
        'criterion': 'bic',
        'nu': 'auto',
        'tol': 1e-3
    }

    df = pd.read_csv(args.data_direc + args.filename).dropna()
    data = df.iloc[:, 0:-1].values.astype(float)
    label = df['Label'].astype(int).to_numpy()
    print('data: ', data.shape)
    print('label: ', label.shape)

    slidingWindow = find_length_rank(data, rank=1)

    models = ['poly', 'sgd']
    criterions = ['bic', 'aic']
    for model in models:
      for criterion in criterions:
        print(f"\nRunning model: {model} , criterion: {criterion}")
        Custom_AD_HP['model'] = model
        Custom_AD_HP['criterion'] = criterion
        clf = OCSVM(**Custom_AD_HP)
        output = clf.predict(data)
        pred = output   # output has already the predictions
        evaluation_result = get_metrics(output, label, slidingWindow=slidingWindow, pred=pred)
        print('Evaluation Result: ', evaluation_result)
